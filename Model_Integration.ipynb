{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MaAtxakyPhTq",
        "outputId": "78311752-445f-4a50-8122-83a941233d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.12-py3-none-any.whl (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi==2023.7.22 (from roboflow)\n",
            "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==4.0.0 (from roboflow)\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler==0.10.0 (from roboflow)\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting idna==2.10 (from roboflow)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.23.5)\n",
            "Collecting opencv-python-headless==4.8.0.74 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Collecting pyparsing==2.4.7 (from roboflow)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Collecting supervision (from roboflow)\n",
            "  Downloading supervision-0.17.1-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-magic (from roboflow)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.46.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "Requirement already satisfied: scipy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (1.11.4)\n",
            "Installing collected packages: python-magic, python-dotenv, pyparsing, opencv-python-headless, idna, cycler, chardet, certifi, supervision, requests-toolbelt, roboflow\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.8.1.78\n",
            "    Uninstalling opencv-python-headless-4.8.1.78:\n",
            "      Successfully uninstalled opencv-python-headless-4.8.1.78\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.11.17\n",
            "    Uninstalling certifi-2023.11.17:\n",
            "      Successfully uninstalled certifi-2023.11.17\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed certifi-2023.7.22 chardet-4.0.0 cycler-0.10.0 idna-2.10 opencv-python-headless-4.8.0.74 pyparsing-2.4.7 python-dotenv-1.0.0 python-magic-0.4.27 requests-toolbelt-1.0.0 roboflow-1.1.12 supervision-0.17.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cycler",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y529fanDrHS",
        "outputId": "d2fd7d58-dded-4f56-f579-4cba301e609e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell-Phone-Detection-Model**"
      ],
      "metadata": {
        "id": "sEdJg0OsP5KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"YOUR_API_KEY\")\n",
        "project = rf.workspace().project(\"YOUR_PROJECT_NAME\")\n",
        "model = project.version(14).model\n",
        "\n",
        "# Inference on a local image\n",
        "cell_predict=model.predict(\"IMAGE_PATH\", confidence=10, overlap=30).json()\n",
        "cell_predict=cell_predict['predictions']\n",
        "print(\"Count of Cell-Phones and Persons :\",len(cell_predict))\n",
        "\n",
        "count_cell_phones = 0\n",
        "for obj in cell_predict:\n",
        "  if obj['class'] == 'cell-phones':\n",
        "    count_cell_phones += 1\n",
        "\n",
        "print(\"Count of Cell-Phones :\",count_cell_phones)\n",
        "\n",
        "print(cell_predict)\n",
        "# visualize your prediction\n",
        "# model.predict(\"your_image.jpg\", confidence=40, overlap=30).save(\"prediction.jpg\")\n",
        "\n",
        "# infer on an image hosted elsewhere\n",
        "# print(model.predict(\"URL_OF_YOUR_IMAGE\", hosted=True, confidence=40, overlap=30).json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W56FceR1P1Ac",
        "outputId": "7728afd8-eaec-44dc-9a18-544ef3f3d5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Count of Cell-Phones and Persons : 12\n",
            "Count of Cell-Phones : 6\n",
            "[{'x': 606.0, 'y': 470.0, 'width': 36.0, 'height': 24.0, 'confidence': 0.8351991772651672, 'class': 'cell-phones', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 896.5, 'y': 445.0, 'width': 39.0, 'height': 20.0, 'confidence': 0.81318598985672, 'class': 'cell-phones', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 190.5, 'y': 630.5, 'width': 175.0, 'height': 157.0, 'confidence': 0.8024405241012573, 'class': 'person', 'class_id': 1, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 144.5, 'y': 585.5, 'width': 43.0, 'height': 37.0, 'confidence': 0.801435112953186, 'class': 'cell-phones', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 582.0, 'y': 394.0, 'width': 38.0, 'height': 20.0, 'confidence': 0.7887106537818909, 'class': 'cell-phones', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 583.5, 'y': 268.5, 'width': 25.0, 'height': 11.0, 'confidence': 0.7824618816375732, 'class': 'cell-phones', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 669.0, 'y': 506.5, 'width': 162.0, 'height': 165.0, 'confidence': 0.771395206451416, 'class': 'person', 'class_id': 1, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 644.5, 'y': 384.5, 'width': 103.0, 'height': 119.0, 'confidence': 0.6078111529350281, 'class': 'person', 'class_id': 1, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 637.5, 'y': 257.0, 'width': 89.0, 'height': 72.0, 'confidence': 0.5342174172401428, 'class': 'person', 'class_id': 1, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 985.0, 'y': 459.0, 'width': 136.0, 'height': 128.0, 'confidence': 0.39873164892196655, 'class': 'person', 'class_id': 1, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 762.5, 'y': 365.5, 'width': 33.0, 'height': 23.0, 'confidence': 0.31074094772338867, 'class': 'cell-phones', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 802.0, 'y': 358.0, 'width': 100.0, 'height': 96.0, 'confidence': 0.197199285030365, 'class': 'person', 'class_id': 1, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Table-detection-Model**"
      ],
      "metadata": {
        "id": "DEYdnXu2Rodn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = project.version(1).model\n",
        "\n",
        "# infer on a local image\n",
        "tables=model.predict(\"IMAGE_PATH\", confidence=10, overlap=30).json()\n",
        "tables=tables['predictions']\n",
        "\n",
        "print(len(tables))\n",
        "print(tables)\n",
        "tables = sorted(tables, key = lambda a: (a['y'], a['x']))\n",
        "print(tables)\n",
        "\n",
        "# visualize your prediction\n",
        "# print(model.predict(\"img.png\", confidence=40, overlap=30).save(\"prediction.jpg\"))\n",
        "\n",
        "# infer on an image hosted elsewhere\n",
        "# print(model.predict(\"URL_OF_YOUR_IMAGE\", hosted=True, confidence=40, overlap=30).json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psfIxcQ_RvCu",
        "outputId": "ac76bb00-3708-4eed-83b6-3034507ecf72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "30\n",
            "[{'x': 1051, 'y': 384, 'width': 114, 'height': 41, 'confidence': 0.9810207486152649, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 1049, 'y': 507, 'width': 170, 'height': 75, 'confidence': 0.9734432697296143, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 1168, 'y': 442, 'width': 100, 'height': 63, 'confidence': 0.973253607749939, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 352, 'y': 256, 'width': 204, 'height': 38, 'confidence': 0.9653865694999695, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 1201, 'y': 608, 'width': 114, 'height': 92, 'confidence': 0.96361243724823, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 498, 'y': 218, 'width': 172, 'height': 29, 'confidence': 0.9628551006317139, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 74, 'y': 200, 'width': 112, 'height': 22, 'confidence': 0.9620078802108765, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 272, 'y': 191, 'width': 152, 'height': 29, 'confidence': 0.9619408845901489, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 897, 'y': 435, 'width': 162, 'height': 55, 'confidence': 0.9615553617477417, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 629, 'y': 285, 'width': 202, 'height': 36, 'confidence': 0.9589225649833679, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 310, 'y': 222, 'width': 180, 'height': 28, 'confidence': 0.9573307037353516, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 82, 'y': 227, 'width': 120, 'height': 23, 'confidence': 0.956876277923584, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 556, 'y': 250, 'width': 184, 'height': 30, 'confidence': 0.9562891721725464, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 447, 'y': 187, 'width': 154, 'height': 27, 'confidence': 0.9484177231788635, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 411, 'y': 295, 'width': 234, 'height': 49, 'confidence': 0.9459751844406128, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 477, 'y': 353, 'width': 278, 'height': 57, 'confidence': 0.9448916912078857, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 108, 'y': 370, 'width': 216, 'height': 61, 'confidence': 0.9424313306808472, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 93, 'y': 309, 'width': 182, 'height': 43, 'confidence': 0.9373764395713806, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 87, 'y': 262, 'width': 158, 'height': 29, 'confidence': 0.9311046600341797, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 709, 'y': 329, 'width': 210, 'height': 47, 'confidence': 0.9280334115028381, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 776, 'y': 263, 'width': 120, 'height': 24, 'confidence': 0.9131443500518799, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 502, 'y': 420, 'width': 172, 'height': 72, 'confidence': 0.867962121963501, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 618, 'y': 196, 'width': 108, 'height': 23, 'confidence': 0.8625779151916504, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 806, 'y': 375, 'width': 212, 'height': 46, 'confidence': 0.8451895713806152, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 943, 'y': 338, 'width': 102, 'height': 30, 'confidence': 0.8403595685958862, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 156, 'y': 470, 'width': 312, 'height': 99, 'confidence': 0.8057506680488586, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 687, 'y': 221, 'width': 106, 'height': 22, 'confidence': 0.7968000769615173, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 860, 'y': 296, 'width': 124, 'height': 30, 'confidence': 0.7939702272415161, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 811, 'y': 627, 'width': 302, 'height': 150, 'confidence': 0.6721152663230896, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 147, 'y': 561, 'width': 278, 'height': 52, 'confidence': 0.10765284299850464, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}]\n",
            "[{'x': 447, 'y': 187, 'width': 154, 'height': 27, 'confidence': 0.9484177231788635, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 272, 'y': 191, 'width': 152, 'height': 29, 'confidence': 0.9619408845901489, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 618, 'y': 196, 'width': 108, 'height': 23, 'confidence': 0.8625779151916504, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 74, 'y': 200, 'width': 112, 'height': 22, 'confidence': 0.9620078802108765, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 498, 'y': 218, 'width': 172, 'height': 29, 'confidence': 0.9628551006317139, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 687, 'y': 221, 'width': 106, 'height': 22, 'confidence': 0.7968000769615173, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 310, 'y': 222, 'width': 180, 'height': 28, 'confidence': 0.9573307037353516, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 82, 'y': 227, 'width': 120, 'height': 23, 'confidence': 0.956876277923584, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 556, 'y': 250, 'width': 184, 'height': 30, 'confidence': 0.9562891721725464, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 352, 'y': 256, 'width': 204, 'height': 38, 'confidence': 0.9653865694999695, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 87, 'y': 262, 'width': 158, 'height': 29, 'confidence': 0.9311046600341797, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 776, 'y': 263, 'width': 120, 'height': 24, 'confidence': 0.9131443500518799, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 629, 'y': 285, 'width': 202, 'height': 36, 'confidence': 0.9589225649833679, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 411, 'y': 295, 'width': 234, 'height': 49, 'confidence': 0.9459751844406128, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 860, 'y': 296, 'width': 124, 'height': 30, 'confidence': 0.7939702272415161, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 93, 'y': 309, 'width': 182, 'height': 43, 'confidence': 0.9373764395713806, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 709, 'y': 329, 'width': 210, 'height': 47, 'confidence': 0.9280334115028381, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 943, 'y': 338, 'width': 102, 'height': 30, 'confidence': 0.8403595685958862, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 477, 'y': 353, 'width': 278, 'height': 57, 'confidence': 0.9448916912078857, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 108, 'y': 370, 'width': 216, 'height': 61, 'confidence': 0.9424313306808472, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 806, 'y': 375, 'width': 212, 'height': 46, 'confidence': 0.8451895713806152, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 1051, 'y': 384, 'width': 114, 'height': 41, 'confidence': 0.9810207486152649, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 502, 'y': 420, 'width': 172, 'height': 72, 'confidence': 0.867962121963501, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 897, 'y': 435, 'width': 162, 'height': 55, 'confidence': 0.9615553617477417, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 1168, 'y': 442, 'width': 100, 'height': 63, 'confidence': 0.973253607749939, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 156, 'y': 470, 'width': 312, 'height': 99, 'confidence': 0.8057506680488586, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 1049, 'y': 507, 'width': 170, 'height': 75, 'confidence': 0.9734432697296143, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 147, 'y': 561, 'width': 278, 'height': 52, 'confidence': 0.10765284299850464, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 1201, 'y': 608, 'width': 114, 'height': 92, 'confidence': 0.96361243724823, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}, {'x': 811, 'y': 627, 'width': 302, 'height': 150, 'confidence': 0.6721152663230896, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "for table in tables:\n",
        "  table['index'] = index\n",
        "  index += 1\n",
        "\n",
        "print(tables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sqmN9PDKDon",
        "outputId": "3a1623fb-c766-404c-b090-b2453c82966f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'x': 447, 'y': 187, 'width': 154, 'height': 27, 'confidence': 0.9484177231788635, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 0}, {'x': 272, 'y': 191, 'width': 152, 'height': 29, 'confidence': 0.9619408845901489, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 1}, {'x': 618, 'y': 196, 'width': 108, 'height': 23, 'confidence': 0.8625779151916504, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 2}, {'x': 74, 'y': 200, 'width': 112, 'height': 22, 'confidence': 0.9620078802108765, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 3}, {'x': 498, 'y': 218, 'width': 172, 'height': 29, 'confidence': 0.9628551006317139, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 4}, {'x': 687, 'y': 221, 'width': 106, 'height': 22, 'confidence': 0.7968000769615173, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 5}, {'x': 310, 'y': 222, 'width': 180, 'height': 28, 'confidence': 0.9573307037353516, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 6}, {'x': 82, 'y': 227, 'width': 120, 'height': 23, 'confidence': 0.956876277923584, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 7}, {'x': 556, 'y': 250, 'width': 184, 'height': 30, 'confidence': 0.9562891721725464, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 8}, {'x': 352, 'y': 256, 'width': 204, 'height': 38, 'confidence': 0.9653865694999695, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 9}, {'x': 87, 'y': 262, 'width': 158, 'height': 29, 'confidence': 0.9311046600341797, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 10}, {'x': 776, 'y': 263, 'width': 120, 'height': 24, 'confidence': 0.9131443500518799, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 11}, {'x': 629, 'y': 285, 'width': 202, 'height': 36, 'confidence': 0.9589225649833679, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 12}, {'x': 411, 'y': 295, 'width': 234, 'height': 49, 'confidence': 0.9459751844406128, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 13}, {'x': 860, 'y': 296, 'width': 124, 'height': 30, 'confidence': 0.7939702272415161, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 14}, {'x': 93, 'y': 309, 'width': 182, 'height': 43, 'confidence': 0.9373764395713806, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 15}, {'x': 709, 'y': 329, 'width': 210, 'height': 47, 'confidence': 0.9280334115028381, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 16}, {'x': 943, 'y': 338, 'width': 102, 'height': 30, 'confidence': 0.8403595685958862, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 17}, {'x': 477, 'y': 353, 'width': 278, 'height': 57, 'confidence': 0.9448916912078857, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 18}, {'x': 108, 'y': 370, 'width': 216, 'height': 61, 'confidence': 0.9424313306808472, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 19}, {'x': 806, 'y': 375, 'width': 212, 'height': 46, 'confidence': 0.8451895713806152, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 20}, {'x': 1051, 'y': 384, 'width': 114, 'height': 41, 'confidence': 0.9810207486152649, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 21}, {'x': 502, 'y': 420, 'width': 172, 'height': 72, 'confidence': 0.867962121963501, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 22}, {'x': 897, 'y': 435, 'width': 162, 'height': 55, 'confidence': 0.9615553617477417, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 23}, {'x': 1168, 'y': 442, 'width': 100, 'height': 63, 'confidence': 0.973253607749939, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 24}, {'x': 156, 'y': 470, 'width': 312, 'height': 99, 'confidence': 0.8057506680488586, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 25}, {'x': 1049, 'y': 507, 'width': 170, 'height': 75, 'confidence': 0.9734432697296143, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 26}, {'x': 147, 'y': 561, 'width': 278, 'height': 52, 'confidence': 0.10765284299850464, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 27}, {'x': 1201, 'y': 608, 'width': 114, 'height': 92, 'confidence': 0.96361243724823, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 28}, {'x': 811, 'y': 627, 'width': 302, 'height': 150, 'confidence': 0.6721152663230896, 'class': 'Table', 'class_id': 0, 'image_path': '/content/drive/MyDrive/STC_Computer_Vision/Test Inference Image and Video-YOLOv8m/Class_2_Video_2_mp4-25.jpg', 'prediction_type': 'ObjectDetectionModel', 'index': 29}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tables on which phones are detected**"
      ],
      "metadata": {
        "id": "oiP06D8VaJe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store the indexes of tables where cell phones are detected\n",
        "cell_phones_in_tables = []    # cell_phone coords, table coords, table index\n",
        "table_indexes = []\n",
        "\n",
        "for cell_phone in cell_predict:\n",
        "  if(cell_phone['class'] == 'cell-phones'):\n",
        "    for table in tables:\n",
        "      top_left = [table['x'] - table['width']/2, table['y'] - table['height']/2]\n",
        "      top_right = [table['x'] + table['width']/2, table['y'] - table['height']/2]\n",
        "      bottom_left = [table['x'] - table['width']/2, table['y'] + table['height']/2]\n",
        "      bottom_right = [table['x'] + table['width']/2, table['y'] + table['height']/2]\n",
        "\n",
        "      if cell_phone['x'] >= top_left[0] and cell_phone['x'] <= top_right[0] and cell_phone['y'] >= top_left[1] and cell_phone['y'] <= bottom_left[1]:\n",
        "\n",
        "        cell_phones_in_tables.append([cell_phone['x'], cell_phone['y'], cell_phone['width'], cell_phone['height'], table['x'], table['y'], table['width'], table['height'], table['index']])\n",
        "        table_indexes.append(table['index'])\n",
        "\n",
        "print(table_indexes)\n",
        "print(cell_phones_in_tables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz0EpkLRKqnR",
        "outputId": "d31b066e-3deb-48fa-e6c4-de886b189d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23, 27, 22, 12, 20]\n",
            "[[896.5, 445.0, 39.0, 20.0, 897, 435, 162, 55, 23], [144.5, 585.5, 43.0, 37.0, 147, 561, 278, 52, 27], [582.0, 394.0, 38.0, 20.0, 502, 420, 172, 72, 22], [583.5, 268.5, 25.0, 11.0, 629, 285, 202, 36, 12], [762.5, 365.5, 33.0, 23.0, 806, 375, 212, 46, 20]]\n"
          ]
        }
      ]
    }
  ]
}